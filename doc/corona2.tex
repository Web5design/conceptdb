\documenttype{article}

\title{Corona redesign -- March 2011}
\author{Rob Speer}

\begin{document}
\maketitle

CORONA did not work as planned. Although I didn't discover this in time to
finish the paper, its emergent effects on large networks turned out to
undesirable.  For example: either (without normalization) agents that cast a
lot of votes accumulate lots of power, or (with normalization) every node can
cause huge swings of reliability in its sparsely-connected neighbors,
especially by casting negative votes.

On real ConceptNet data, the first case leads to the unreliable Verbosity
having thousands of times more influence than other knowledge sources, and the
second case means that good critics manage to negate themselves out of
existence.

The main sources of these problems were:

- Influence congregates in highly-connected areas of the graph. When influence
  is the same thing as reliability, there is no meaningful global scale for
  reliability.
- Negations require subverting the mathematical machinery that made the system
  work in the first place. The combined effect of many negations can send the
  model into Bizarro Minus World, yielding results that make the opposite of
  sense.
- Negations also created "retribution": if node A downvotes node B, that is the
  same as node B downvoting node A. If a node is a person, that person would
  never want to downvote anything.

\section{Weighted averaging}

The key redesign I have in mind is to separate the vote that is being cast from
the strength of that vote. These "votes" are assertions of how reliable node A
thinks node B should be, from 0 to 1. A vote of 0 means you think it's
unreliable. A vote of 1 means you think it's reliable. Votes in between are
also possible and helpful, of course (although, from a voting theory point of
view, a rational, self-interested voter will only cast votes of 0 or 1).

Because of this averaging process, negative reliability is no longer necessary.
Unreliability is now indicated by values near 0.

Terminology: we have nodes with different amounts of reliability casting votes
about different amounts of reliability. Let us refer to these as the *node
weight* and the *edge weight* respectively. Then, the reliability of a node
comes from two sources:

- The average edge weight of its predecessors, weighted by their node weight
  (How reliable do other nodes think you are?)
- The average node weight of its successors, weighted by their edge weight
  (When you say whether other nodes are reliable, how good is your word?)

Notice the exchange of "node" and "edge" there: these effects are dual to each
other. 

Suppose node A upvotes node B (says that B should have high reliability). Node
B will eventually send back a dual message about how good this vote was, based
on the score node B ends up receiving, and this will be averaged into the
reliability of node A. This is the behavior we want in a very common case:
agreement leads to reliability.

Now suppose that node A downvotes node C, saying that C should have reliability
0. C does not drag down A with it, because the dual message from node C has
*weight* 0.

\subsection{Weights vs. probabilities}

Although these numbers fall between 0 and 1, I still do not intend to treat
them as probabilities, because the events described have no known probabilistic
model, and because reconciling different assessments of probability for the
same event is an unnecessarily complex problem. These numbers should instead be
considered as fuzzy truth values.

These values can be considered directly as fuzzy logic truth values, instead of
requiring mapping through a function $f$.

The justification graph, as before, may contain disjunctions or conjunctions of
justifications. Here is the mapping between those and fuzzy logic:

\begin{itemize}
\item Disjunctions of justifications are combined in a weighted average, as
described above.
\item Conjunctions are implemented using the Hamacher product $H(a, b) =
ab/(a+b-ab)$, as in the original paper. The advantage of the Hamacher product
over the standard arithmetic product is that conjunctions of low-reliability
nodes are punished only linearly, not quadratically. $H(a, a) = o(a)$ as $a
\rightarrow 0$, while $a \cdot a = o(a^2)$.
\item Negations are no longer necessary as a separate mechanism; a node can decrease the reliability of another node by giving it a vote of 0 or a sufficiently low number.
\end{itemize}

\section{Combining justifications and a prior}

Nodes start with a defined level of reliability, which can be set to an
arbitrary value. Each node starts with the equivalent of one vote for it at
that level of reliability. Because the weighted average and the Hamacher
product are approximately linear, changing the prior should only change the
scale factor of the results.

\end{document}
